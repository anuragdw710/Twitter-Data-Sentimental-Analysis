# Twitter-Data-Sentimental-Analysis
create a system which flume/Sqoop for data injection in HDFS and then use hive for processing that data and use Pig for data cleaning after that output you get just stored it in Hbase for quick data extraction.  Twitter Streaming using Flume. Use hbase to make tabular storage structure for your tweets and then you can use hive to query them / pig to clean the data (trim, pad etc).  Twitter Data Sentimental Analysis  Fetch structured &amp; unstructured data sets from various sources like Social Media Sites, Web Server &amp; structured source like MySQL, Oracle &amp; others and dump it into HDFS and then analyze the same datasets using PIG,HQL queries &amp; MapReduce technologies to gain proficiency in Hadoop related stack &amp; its ecosystem tools.  Data Analysis Steps in :  1. Dump XML &amp; JSON datasets into HDFS. 2. Convert semi-structured data formats(JSON &amp; XML) into structured format using Pig,Hive &amp; MapReduce. 3. Push the data set into PIG &amp; Hive environment for further analysis. 4. Writing Hive queries to push the output into relational database(RDBMS) using Sqoop. 5. Renders the result in Box Plot, Bar Graph &amp; others using R &amp; Python integration with Hadoop.
